{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "neptune": {
      "notebookId": "94d79b70-3771-405b-b534-feb578dd3a84"
    },
    "colab": {
      "name": "new_model_9513_Copy1 (4).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DqWHWiLZiJU",
        "colab_type": "code",
        "outputId": "15f0e27c-e55a-46f4-d471-3fa707bdc730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "9rKzXsBLZSNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, gc, warnings, random, datetime, math\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "########################### Model\n",
        "import lightgbm as lgb\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "P45UqBzIZSNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Helpers\n",
        "#################################################################################\n",
        "## Seeder\n",
        "# :seed to make all processes deterministic     # type: int\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "## Memory Reducer\n",
        "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
        "# :verbose                                        # type: bool\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L5MZ7_uZSN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Vars\n",
        "#################################################################################\n",
        "SEED = 42\n",
        "seed_everything(SEED)\n",
        "TARGET = 'isFraud'\n",
        "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJci9kRZSN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def values_normalization(dt_df, periods, columns):\n",
        "    for period in periods:\n",
        "        for col in columns:\n",
        "            new_col = col +'_'+ period\n",
        "            dt_df[col] = dt_df[col].astype(float)  \n",
        "\n",
        "            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n",
        "            temp_min.index = temp_min[period].values\n",
        "            temp_min = temp_min['min'].to_dict()\n",
        "\n",
        "            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n",
        "            temp_max.index = temp_max[period].values\n",
        "            temp_max = temp_max['max'].to_dict()\n",
        "\n",
        "            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n",
        "            temp_mean.index = temp_mean[period].values\n",
        "            temp_mean = temp_mean['mean'].to_dict()\n",
        "\n",
        "            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n",
        "            temp_std.index = temp_std[period].values\n",
        "            temp_std = temp_std['std'].to_dict()\n",
        "\n",
        "            dt_df['temp_min'] = dt_df[period].map(temp_min)\n",
        "            dt_df['temp_max'] = dt_df[period].map(temp_max)\n",
        "            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n",
        "            dt_df['temp_std'] = dt_df[period].map(temp_std)\n",
        "\n",
        "            dt_df[new_col + '_min_max'] = (dt_df[col] - dt_df['temp_min']) / (dt_df['temp_max'] - dt_df['temp_min'])\n",
        "            dt_df[new_col +  '_std_score'] = (dt_df[col] - dt_df['temp_mean']) / (dt_df['temp_std'])\n",
        "            del dt_df['temp_min'], dt_df['temp_max'], dt_df['temp_mean'], dt_df['temp_std']\n",
        "    return dt_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70GkilpZZSOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frequency_encoding(train_df, test_df, columns, self_encoding=False):\n",
        "    for col in columns:\n",
        "        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n",
        "        if self_encoding:\n",
        "            train_df[col] = train_df[col].map(fq_encode)\n",
        "            test_df[col]  = test_df[col].map(fq_encode)            \n",
        "        else:\n",
        "            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
        "            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n",
        "    return train_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5s6tr2xZSOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n",
        "                                 with_proportions=True, only_proportions=False):\n",
        "    for period in periods:\n",
        "        for col in columns:\n",
        "            new_col = col + '_' + period\n",
        "            train_df[new_col] = train_df[col].astype(str) + '_' + train_df[period].astype(str)\n",
        "            test_df[new_col]  = test_df[col].astype(str) + '_' + test_df[period].astype(str)\n",
        "\n",
        "            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n",
        "            fq_encode = temp_df[new_col].value_counts().to_dict()\n",
        "\n",
        "            train_df[new_col] = train_df[new_col].map(fq_encode)\n",
        "            test_df[new_col]  = test_df[new_col].map(fq_encode)\n",
        "            \n",
        "            if only_proportions:\n",
        "                train_df[new_col] = train_df[new_col] / train_df[period + '_total']\n",
        "                test_df[new_col]  = test_df[new_col] / test_df[period + '_total']\n",
        "\n",
        "            if with_proportions:\n",
        "                train_df[new_col + '_proportions'] = train_df[new_col] / train_df[period + '_total']\n",
        "                test_df[new_col + '_proportions']  = test_df[new_col] / test_df[period + '_total']\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VDeEIhRZSOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n",
        "    \n",
        "    for main_column in main_columns:  \n",
        "        for col in uids:\n",
        "            for agg_type in aggregations:                  \n",
        "                \n",
        "                new_col_name = col + '_' + main_column + '_' + agg_type\n",
        "                \n",
        "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
        "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
        "                                                        columns={agg_type: new_col_name})\n",
        "\n",
        "                temp_df.index = list(temp_df[col])\n",
        "                temp_df = temp_df[new_col_name].to_dict()   \n",
        "\n",
        "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
        "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
        "                \n",
        "    return train_df, test_df\n",
        "\n",
        "def uid_aggregation_and_normalization(train_df, test_df, main_columns, uids, aggregations):\n",
        "    for main_column in main_columns:  \n",
        "        for col in uids:\n",
        "            \n",
        "            new_norm_col_name = col + '_' + main_column + '_std_norm'\n",
        "            norm_cols = []\n",
        "            \n",
        "            for agg_type in aggregations:\n",
        "                new_col_name = col + '_' + main_column + '_' + agg_type\n",
        "                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
        "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
        "                                                        columns={agg_type: new_col_name})\n",
        "\n",
        "                temp_df.index = list(temp_df[col])\n",
        "                temp_df = temp_df[new_col_name].to_dict()   \n",
        "\n",
        "                train_df[new_col_name] = train_df[col].map(temp_df)\n",
        "                test_df[new_col_name]  = test_df[col].map(temp_df)\n",
        "                norm_cols.append(new_col_name)\n",
        "            \n",
        "            train_df[new_norm_col_name] = (train_df[main_column] - train_df[norm_cols[0]]) / train_df[norm_cols[1]]\n",
        "            test_df[new_norm_col_name]  = (test_df[main_column] - test_df[norm_cols[0]]) / test_df[norm_cols[1]]          \n",
        "            \n",
        "            del train_df[norm_cols[0]], train_df[norm_cols[1]]\n",
        "            del test_df[norm_cols[0]], test_df[norm_cols[1]]\n",
        "                                              \n",
        "    return train_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24wR-dGjZSOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_cor_and_remove(train_df, test_df, i_cols, new_columns, remove=False):\n",
        "    # Check correllation\n",
        "    print('Correlations','#'*10)\n",
        "    for col in new_columns:\n",
        "        cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
        "        print(col, cor_cof)\n",
        "\n",
        "    if remove:\n",
        "        print('#'*10)\n",
        "        print('Best options:')\n",
        "        best_fe_columns = []\n",
        "        for main_col in i_cols:\n",
        "            best_option = ''\n",
        "            best_cof = 0\n",
        "            for col in new_columns:\n",
        "                if main_col in col:\n",
        "                    cor_cof = np.corrcoef(train_df[TARGET], train_df[col].fillna(0))[0][1]\n",
        "                    cor_cof = (cor_cof**2)**0.5\n",
        "                    if cor_cof > best_cof:\n",
        "                        best_cof = cor_cof\n",
        "                        best_option = col\n",
        "\n",
        "            print(main_col, best_option, best_cof)            \n",
        "            best_fe_columns.append(best_option)\n",
        "\n",
        "        for col in new_columns:\n",
        "            if col not in best_fe_columns:\n",
        "                del train_df[col], test_df[col]\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axOTAqNIZSOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_pairs(train, test, pairs):\n",
        "    for pair in pairs:\n",
        "\n",
        "        unique_train = []\n",
        "        unique_test = []\n",
        "\n",
        "        print(f'Pair: {pair}')\n",
        "        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n",
        "        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n",
        "\n",
        "        for value in train[pair[0]].unique():\n",
        "            unique_train.append(train[pair[1]][train[pair[0]] == value].value_counts().shape[0])\n",
        "\n",
        "        for value in test[pair[0]].unique():\n",
        "            unique_test.append(test[pair[1]][test[pair[0]] == value].value_counts().shape[0])\n",
        "\n",
        "        pair_values_train = pd.Series(data=unique_train, index=train[pair[0]].unique())\n",
        "        pair_values_test = pd.Series(data=unique_test, index=test[pair[0]].unique())\n",
        "        \n",
        "        print('Filling train...')\n",
        "\n",
        "        for value in pair_values_train[pair_values_train == 1].index:\n",
        "            train.loc[train[pair[0]] == value, pair[1]] = train.loc[train[pair[0]] == value, pair[1]].value_counts().index[0]\n",
        "\n",
        "        print('Filling test...')\n",
        "\n",
        "        for value in pair_values_test[pair_values_test == 1].index:\n",
        "            test.loc[test[pair[0]] == value, pair[1]] = test.loc[test[pair[0]] == value, pair[1]].value_counts().index[0]\n",
        "\n",
        "        print(f'In train{[pair[1]]} there are {train[pair[1]].isna().sum()} NaNs' )\n",
        "        print(f'In test{[pair[1]]} there are {test[pair[1]].isna().sum()} NaNs' )\n",
        "        \n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQLvGkFQZSO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA \n",
        "\n",
        "def pca_made_cols(train, test, pair):\n",
        "    group = ['V' + str(num) for num in range(pair[0], pair[1] + 1)]\n",
        "    pca_df = pd.concat([train_df, test_df])[group].dropna()\n",
        "    pca = PCA().fit(pca_df)\n",
        "    \n",
        "    sums = np.cumsum(pca.explained_variance_ratio_)\n",
        "    num = len([x for x in sums if x <= 0.98])\n",
        "    \n",
        "    if len(group) > num:\n",
        "        sklearn_pca = PCA(n_components = num + 1)\n",
        "        sklearn_pca.fit(pca_df[group])\n",
        "        print(pair, num + 1)\n",
        "        \n",
        "        #train\n",
        "        group_new = sklearn_pca.transform(train[group].dropna())\n",
        "        group_new = pd.DataFrame(group_new)\n",
        "        name_dict = {}\n",
        "\n",
        "        for i in range(group_new.shape[1]):\n",
        "            name_dict[i] = 'V_' + str(pair[0]) + '_' + str(pair[1]) + '_' + str(i)\n",
        "\n",
        "        new_ind_group = group_new.set_index(pd.Index(train[group].dropna().index)).rename(name_dict, axis=1)\n",
        "        train.drop(group, axis=1, inplace=True)\n",
        "        train = pd.concat([train, new_ind_group], join='outer', axis=1)\n",
        "        \n",
        "        # test\n",
        "        group_new = sklearn_pca.transform(test[group].dropna())\n",
        "        group_new = pd.DataFrame(group_new)\n",
        "        \n",
        "        new_ind_group = group_new.set_index(pd.Index(test[group].dropna().index)).rename(name_dict, axis=1)\n",
        "        test.drop(group, axis=1, inplace=True)\n",
        "        test = pd.concat([test, new_ind_group], join='outer', axis=1)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "pairs = [(1,11), (12, 34), (35, 52), (53, 74), (75, 94), (95, 137), (138, 166), (167, 216), (217, 278), (279, 321), (322, 339)]\n",
        "\n",
        "#for pair in pairs:\n",
        "#    train_df, test_df = pca_made_cols(train_df, test_df, pair)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtSdajhRZSPJ",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDLd81nKZSPN",
        "colab_type": "code",
        "outputId": "f7c16d91-d7c0-49c4-87b0-6f2d9a0d2cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "########################### DATA LOAD\n",
        "#################################################################################\n",
        "print('Load Data')\n",
        "train_df = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/ieee/train_transaction.pkl')\n",
        "test_df = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/ieee/test_transaction.pkl')\n",
        "train_identity = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/ieee/train_identity.pkl')\n",
        "test_identity = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/ieee/test_identity.pkl')\n",
        "\n",
        "base_columns = list(train_df.columns) + list(train_identity.columns)\n",
        "\n",
        "print('Shape control:', train_df.shape, test_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load Data\n",
            "Shape control: (590540, 123) (506691, 123)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsEw5CXmZSPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_features = [\n",
        "    'TransactionID','TransactionDT', # These columns are pure noise right now\n",
        "    TARGET,\n",
        "    ]\n",
        "\n",
        "base_columns = [col for col in list(base_columns) if col not in remove_features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVQ99PwRZSPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Device info and identity\n",
        "for df in [train_identity, test_identity]:\n",
        "    ########################### Device info\n",
        "    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
        "    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
        "    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
        "    \n",
        "    ########################### Device info 2\n",
        "    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
        "    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
        "    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
        "    \n",
        "    ########################### Browser\n",
        "    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
        "    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
        "    \n",
        "########################### Merge Identity columns\n",
        "temp_df = train_df[['TransactionID']]\n",
        "temp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\n",
        "del temp_df['TransactionID']\n",
        "train_df = pd.concat([train_df,temp_df], axis=1)\n",
        "    \n",
        "temp_df = test_df[['TransactionID']]\n",
        "temp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\n",
        "del temp_df['TransactionID']\n",
        "test_df = pd.concat([test_df,temp_df], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWr1NpyTZSPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### TransactionDT\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
        "\n",
        "dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
        "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
        "\n",
        "# Let's add temporary \"time variables\" for aggregations\n",
        "# and add normal \"time variables\"\n",
        "for df in [train_df, test_df]:\n",
        "    \n",
        "    # Temporary variables for aggregation\n",
        "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
        "    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n",
        "    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n",
        "    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n",
        "    \n",
        "    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n",
        "    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n",
        "    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n",
        "\n",
        "# Remove temporary features from final list\n",
        "remove_features += ['DT','DT_M','DT_W','DT_D','DT_hour','DT_day_week','DT_day_month']\n",
        "\n",
        "categorical_features = []\n",
        "    \n",
        "# Total transactions per timeblock\n",
        "for col in ['DT_M','DT_W','DT_D']:\n",
        "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "    fq_encode = temp_df[col].value_counts().to_dict()\n",
        "            \n",
        "    train_df[col + '_total'] = train_df[col].map(fq_encode)\n",
        "    test_df[col + '_total'] = test_df[col].map(fq_encode)\n",
        "    \n",
        "    # We can't use it as solo feature\n",
        "    remove_features.append(col + '_total')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2QGFzxCZSP0",
        "colab_type": "code",
        "outputId": "4f9a8f90-500a-459e-c2d3-4b7c81753f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "########################### Card columns \"outliers\"\n",
        "for col in ['card1']: \n",
        "    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n",
        "    valid_card = valid_card[col].value_counts()\n",
        "    valid_card_std = valid_card.values.std()\n",
        "\n",
        "    invalid_cards = valid_card[valid_card<=2]\n",
        "    print('Rare cards', len(invalid_cards))\n",
        "\n",
        "    valid_card = valid_card[valid_card>2]\n",
        "    valid_card = list(valid_card.index)\n",
        "\n",
        "    print('No intersection in Train', len(train_df[~train_df[col].isin(test_df[col])]))\n",
        "    print('Intersection in Train', len(train_df[train_df[col].isin(test_df[col])]))\n",
        "    \n",
        "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
        "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
        "\n",
        "    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n",
        "    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)\n",
        "    print('#'*20)\n",
        "\n",
        "for col in ['card2','card3','card4','card5','card6',]: \n",
        "    print('No intersection in Train', col, len(train_df[~train_df[col].isin(test_df[col])]))\n",
        "    print('Intersection in Train', col, len(train_df[train_df[col].isin(test_df[col])]))\n",
        "    \n",
        "    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n",
        "    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n",
        "    print('#'*20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rare cards 5993\n",
            "No intersection in Train 10396\n",
            "Intersection in Train 580144\n",
            "####################\n",
            "No intersection in Train card2 5012\n",
            "Intersection in Train card2 585528\n",
            "####################\n",
            "No intersection in Train card3 47\n",
            "Intersection in Train card3 590493\n",
            "####################\n",
            "No intersection in Train card4 0\n",
            "Intersection in Train card4 590540\n",
            "####################\n",
            "No intersection in Train card5 7279\n",
            "Intersection in Train card5 583261\n",
            "####################\n",
            "No intersection in Train card6 30\n",
            "Intersection in Train card6 590510\n",
            "####################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PUgFmwEw_EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groups = pd.read_csv('groups (3).csv', index_col='TransactionID')\n",
        "user_id = pd.read_csv('UserID (1).csv', index_col='TransactionID')\n",
        "\n",
        "train_df = train_df.merge(groups, on='TransactionID', how='left')\n",
        "test_df = test_df.merge(groups, on='TransactionID', how='left')\n",
        "\n",
        "\n",
        "train_df = train_df.merge(user_id, on='TransactionID', how='left')\n",
        "test_df = test_df.merge(user_id, on='TransactionID', how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBz9cDHrw-xE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a_lSGJKFeME",
        "colab_type": "code",
        "outputId": "da3e213a-f9cc-4bd2-bc71-cef7fbfda018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>card6</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>dist1</th>\n",
              "      <th>dist2</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>R_emaildomain</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>C3</th>\n",
              "      <th>C4</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C7</th>\n",
              "      <th>C8</th>\n",
              "      <th>C9</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "      <th>D7</th>\n",
              "      <th>D8</th>\n",
              "      <th>D9</th>\n",
              "      <th>...</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_23</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_27</th>\n",
              "      <th>id_28</th>\n",
              "      <th>id_29</th>\n",
              "      <th>id_30</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "      <th>id_33_0</th>\n",
              "      <th>id_33_1</th>\n",
              "      <th>DeviceInfo_device</th>\n",
              "      <th>DeviceInfo_version</th>\n",
              "      <th>id_30_device</th>\n",
              "      <th>id_30_version</th>\n",
              "      <th>id_31_device</th>\n",
              "      <th>DT</th>\n",
              "      <th>DT_M</th>\n",
              "      <th>DT_W</th>\n",
              "      <th>DT_D</th>\n",
              "      <th>DT_hour</th>\n",
              "      <th>DT_day_week</th>\n",
              "      <th>DT_day_month</th>\n",
              "      <th>DT_M_total</th>\n",
              "      <th>DT_W_total</th>\n",
              "      <th>DT_D_total</th>\n",
              "      <th>groups</th>\n",
              "      <th>UserID_proxy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>800657</td>\n",
              "      <td>13926.0</td>\n",
              "      <td>327.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>9524.0</td>\n",
              "      <td>142.0</td>\n",
              "      <td>267648.0</td>\n",
              "      <td>315.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-12-01 00:00:00</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>335</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>137321</td>\n",
              "      <td>12093</td>\n",
              "      <td>5122</td>\n",
              "      <td>group0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>800657</td>\n",
              "      <td>2755.0</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>347386.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>267648.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-12-01 00:00:01</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>335</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>137321</td>\n",
              "      <td>12093</td>\n",
              "      <td>5122</td>\n",
              "      <td>group1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>800657</td>\n",
              "      <td>4663.0</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>719649.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>824959.0</td>\n",
              "      <td>330.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>outlook.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-12-01 00:01:09</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>335</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>137321</td>\n",
              "      <td>12093</td>\n",
              "      <td>5122</td>\n",
              "      <td>group2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>800657</td>\n",
              "      <td>18132.0</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>347386.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>824959.0</td>\n",
              "      <td>476.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yahoo.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-12-01 00:01:39</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>335</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>137321</td>\n",
              "      <td>12093</td>\n",
              "      <td>5122</td>\n",
              "      <td>group9</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>62397</td>\n",
              "      <td>4497.0</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>347386.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>267648.0</td>\n",
              "      <td>420.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>gmail.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>144.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>android 7.0</td>\n",
              "      <td>samsung browser 6.2</td>\n",
              "      <td>32.0</td>\n",
              "      <td>267.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>mobile</td>\n",
              "      <td>samsung sm-g892a build/nrd90m</td>\n",
              "      <td>2220.0</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>samsungsmgabuildnrdm</td>\n",
              "      <td>89290</td>\n",
              "      <td>android</td>\n",
              "      <td>70</td>\n",
              "      <td>samsungbrowser</td>\n",
              "      <td>2017-12-01 00:01:46</td>\n",
              "      <td>12</td>\n",
              "      <td>48</td>\n",
              "      <td>335</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>137321</td>\n",
              "      <td>12093</td>\n",
              "      <td>5122</td>\n",
              "      <td>group14</td>\n",
              "      <td>Hgroup8511</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  ...  DT_D_total   groups  UserID_proxy\n",
              "0        2987000        0          86400  ...        5122   group0           NaN\n",
              "1        2987001        0          86401  ...        5122   group1           NaN\n",
              "2        2987002        0          86469  ...        5122   group2           NaN\n",
              "3        2987003        0          86499  ...        5122   group9           NaN\n",
              "4        2987004        0          86506  ...        5122  group14    Hgroup8511\n",
              "\n",
              "[5 rows x 182 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIska3oUFd6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UsELKBgFdyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVYhbM3MFddD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCnJuO5lZSP7",
        "colab_type": "code",
        "outputId": "3416b7ab-b936-497f-f576-af2066c29a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "train_df['uid_DT'] = (train_df['D1'] - train_df['DT_D']).astype(str) + '__' + train_df['card1'].astype(str)\n",
        "test_df['uid_DT'] = (test_df['D1'] - test_df['DT_D']).astype(str) + '__' + test_df['card1'].astype(str)\n",
        "\n",
        "train_df['uid5'] = train_df['uid_DT'].astype(str)+'_'+train_df['TransactionAmt'].astype(str)\n",
        "test_df['uid5'] = test_df['uid_DT'].astype(str)+'_'+test_df['TransactionAmt'].astype(str)\n",
        "\n",
        "remove_features += ['D1']\n",
        "\n",
        "# Add values remove list\n",
        "new_columns = ['uid_DT', 'uid5', 'groups',\t'UserID_proxy']\n",
        "#remove_features += new_columns\n",
        "\n",
        "print('#'*10)\n",
        "print('Most common uIds:')\n",
        "for col in new_columns:\n",
        "    print('#'*10, col)\n",
        "    print(train_df[col].value_counts()[:10])\n",
        "\n",
        "# Do Global frequency encoding \n",
        "i_cols = ['card1','card2','card3','card5', 'card4', 'card6'] + new_columns\n",
        "#train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)\n",
        "\n",
        "categorical_features += i_cols"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########\n",
            "Most common uIds:\n",
            "########## uid_DT\n",
            "-463.0__15775.0    1414\n",
            "-249.0__9500.0      482\n",
            "-426.0__7919.0      439\n",
            "-335.0__7919.0      410\n",
            "-368.0__7919.0      333\n",
            "-397.0__7919.0      325\n",
            "-398.0__7919.0      286\n",
            "-458.0__7919.0      285\n",
            "-369.0__7919.0      252\n",
            "157.0__12616.0      242\n",
            "Name: uid_DT, dtype: int64\n",
            "########## uid5\n",
            "-463.0__15775.0_106.0    550\n",
            "-463.0__15775.0_110.0    263\n",
            "-463.0__15775.0_125.0    221\n",
            "-175.0__8528.0_59.0      184\n",
            "-463.0__15775.0_75.0     122\n",
            "-463.0__15775.0_105.0    106\n",
            "-358.0__6019.0_100.0      63\n",
            "-495.0__16136.0_0.878     57\n",
            "-463.0__15775.0_55.0      55\n",
            "-242.0__9002.0_15.0       51\n",
            "Name: uid5, dtype: int64\n",
            "########## groups\n",
            "group267171    1413\n",
            "group670        466\n",
            "group210688     344\n",
            "group259665     241\n",
            "group17323      231\n",
            "group28599      215\n",
            "group1759       194\n",
            "group120497     179\n",
            "group9057       173\n",
            "group214512     145\n",
            "Name: groups, dtype: int64\n",
            "########## UserID_proxy\n",
            "Sgroup33      1460\n",
            "group238       577\n",
            "group73        275\n",
            "Hgroup143      207\n",
            "group72        193\n",
            "Hgroup1120     144\n",
            "group495       143\n",
            "group404       134\n",
            "group147       129\n",
            "group789       125\n",
            "Name: UserID_proxy, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENFbcznNZSQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clip Values\n",
        "train_df['TransactionAmt'] = train_df['TransactionAmt'].clip(0,5000)\n",
        "test_df['TransactionAmt']  = test_df['TransactionAmt'].clip(0,5000)\n",
        "\n",
        "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
        "# In our dialog with a model we are telling to trust or not to these values   \n",
        "train_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
        "test_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
        "\n",
        "# For our model current TransactionAmt is a noise\n",
        "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
        "# (even if features importances are telling contrariwise)\n",
        "# There are many unique values and model doesn't generalize well\n",
        "# Lets do some aggregations\n",
        "i_cols = ['TransactionAmt']\n",
        "uids = ['card1','card2','card3','card5', 'uid_DT', 'groups',\t'UserID_proxy']\n",
        "aggregations = ['mean']\n",
        "\n",
        "# uIDs aggregations\n",
        "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
        " \n",
        "# # TransactionAmt Normalization\n",
        "periods = ['DT_D','DT_W','DT_M']\n",
        "for df in [train_df, test_df]:\n",
        "    df = values_normalization(df, periods, i_cols)\n",
        "\n",
        "# Product type\n",
        "train_df['product_type'] = train_df['ProductCD'].astype(str)+'_'+train_df['TransactionAmt'].astype(str)\n",
        "test_df['product_type'] = test_df['ProductCD'].astype(str)+'_'+test_df['TransactionAmt'].astype(str)\n",
        "\n",
        "i_cols = ['product_type']\n",
        "periods = ['DT_D','DT_W','DT_M']\n",
        "train_df, test_df = timeblock_frequency_encoding(train_df, test_df, periods, i_cols, \n",
        "                                                 with_proportions=False, only_proportions=True)\n",
        "#train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)\n",
        "\n",
        "categorical_features += i_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1tjuCuxZSQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_cols = ['D'+str(i) for i in range(2,16)]\n",
        "uids = ['uid_DT', 'groups',\t'UserID_proxy']\n",
        "aggregations = ['mean', 'std']\n",
        "\n",
        "####### uIDs aggregations\n",
        "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
        "\n",
        "####### Cleaning Neagtive values and columns transformations\n",
        "for df in [train_df, test_df]:\n",
        "\n",
        "    for col in i_cols:\n",
        "        df[col] = df[col].clip(0) \n",
        "    \n",
        "    # Lets transform D8 and D9 column\n",
        "    # As we almost sure it has connection with hours\n",
        "    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n",
        "    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n",
        "    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n",
        "    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n",
        "    df['D8'] = df['D8'].fillna(-1).astype(int)\n",
        "\n",
        "# ####### Values Normalization\n",
        "i_cols.remove('D2')\n",
        "i_cols.remove('D9')\n",
        "periods = ['DT_D','DT_W','DT_M']\n",
        "for df in [train_df, test_df]:\n",
        "    df = values_normalization(df, periods, i_cols)\n",
        "\n",
        "for col in ['D2']:\n",
        "    for df in [train_df, test_df]:\n",
        "        df[col + '_scaled'] = df[col] / df[col].max()\n",
        "\n",
        "# i_cols = ['D'+str(i) for i in range(1,16)]\n",
        "\n",
        "# train_df.drop(i_cols, axis=1, inplace=True)\n",
        "# test_df.drop(i_cols, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwin_o9qZSQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tonantheadholddict = {'C1':2000,'C2':2000,'C4':1000,'C6':1000,'C8':800,'C9':240,\n",
        "# 'C14':240,'C10':520,'C11':520,'C12':520,'C13':1000}\n",
        "\n",
        "# for col,thod in tonantheadholddict.items():\n",
        "#     train_df.loc[train_df[col]>= thod, col] = np.nan\n",
        "#     test_df.loc[test_df[col]>= thod, col] = np.nan\n",
        "\n",
        "########################### C Columns\n",
        "i_cols = ['C'+str(i) for i in range(1,15)]\n",
        "uids = ['uid_DT', 'groups',\t'UserID_proxy']\n",
        "aggregations = ['mean', 'std']\n",
        "\n",
        "####### uIDs aggregations\n",
        "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)\n",
        "\n",
        "categorical_features += i_cols\n",
        "\n",
        "####### Clip max values\n",
        "for df in [train_df, test_df]:\n",
        "    for col in i_cols:\n",
        "        max_value = train_df[train_df['DT_M']==train_df['DT_M'].max()][col].max()\n",
        "        df[col] = df[col].clip(None,max_value) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yi2m51PZSQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_cols = ['id_02__id_20', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
        "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1',\n",
        "                'M2__M3', 'D8__D9', 'D11__DeviceInfo', 'id_02__D8', 'id_19__id_20']\n",
        "\n",
        "# Some arbitrary features interaction\n",
        "for feature in i_cols:\n",
        "\n",
        "    f1, f2 = feature.split('__')\n",
        "    train_df[feature] = train_df[f1].astype(str) + '_' + train_df[f2].astype(str)\n",
        "    test_df[feature] = test_df[f1].astype(str) + '_' + test_df[f2].astype(str)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(list(train_df[feature].astype(str).values) + list(test_df[feature].astype(str).values))\n",
        "    train_df[feature] = le.transform(list(train_df[feature].astype(str).values))\n",
        "    test_df[feature] = le.transform(list(test_df[feature].astype(str).values))\n",
        "    \n",
        "#train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=True)\n",
        "categorical_features += i_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgMbkiIXZSQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_cols = [\n",
        "          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n",
        "          'id_30','id_30_device','id_30_version',\n",
        "          'id_31','id_31_device',\n",
        "          'id_33',\n",
        "         ]\n",
        "\n",
        "####### Global Self frequency encoding\n",
        "# self_encoding=True because \n",
        "# we don't need original values anymore\n",
        "#train_df, test_df = frequency_encoding(train_df, test_df, i_cols, self_encoding=False)\n",
        "categorical_features += i_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWjMwhu2ZSRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, test_df = uid_aggregation(train_df, test_df, ['TransactionDT'], ['uid5'],['count', 'std'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP6nkQzghNB4",
        "colab_type": "code",
        "outputId": "01c7dd85-82d7-4433-b11a-df957d44e466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train_df['id_19'].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      NaN\n",
              "1      NaN\n",
              "2      NaN\n",
              "3      NaN\n",
              "4    542.0\n",
              "Name: id_19, dtype: float16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-4gGLDYhMhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "groups = pd.read_csv('/content/drive/My Drive/newroupingfeatures_train_test.csv')\n",
        "\n",
        "addfeatures = ['TransactionID', 'count_group','skew_Amt_group','unique_Amt_group','var_Amt_group','unique_adr1_group','amplitude_dist1_group',\n",
        "               'unique_Pemail_group','Amtisfirst_group','Amt_timeslowest_first_group','amplitude_Time_group']\n",
        "\n",
        "train_df = train_df.merge(groups[addfeatures], on='TransactionID', how='left')\n",
        "test_df = test_df.merge(groups[addfeatures], on='TransactionID', how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gfUW3Q27BIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i_cols = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'DeviceInfo', 'id_31', 'id_19__id_20', 'addr1', 'addr2']\n",
        "uids = ['UserID_proxy']\n",
        "aggregations = ['nunique']\n",
        "\n",
        "####### uIDs aggregations\n",
        "train_df, test_df = uid_aggregation(train_df, test_df, i_cols, uids, aggregations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqCEF4Cp7A6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vho9F-_7AtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khn0TFtE50hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IEZ8qTl50NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30f8KTYmZSRL",
        "colab_type": "code",
        "outputId": "b944dac6-24c8-420b-982f-622b6e5ced05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "########################### Encode Str columns\n",
        "# For all such columns (probably not)\n",
        "# we already did frequency encoding (numeric feature)\n",
        "# so we will use astype('category') here\n",
        "for col in list(train_df):\n",
        "    if train_df[col].dtype=='O':\n",
        "        print(col)\n",
        "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
        "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
        "        \n",
        "        train_df[col] = train_df[col].astype(str)\n",
        "        test_df[col] = test_df[col].astype(str)\n",
        "        \n",
        "        le = LabelEncoder()\n",
        "        le.fit(list(train_df[col])+list(test_df[col]))\n",
        "        train_df[col] = le.transform(train_df[col])\n",
        "        test_df[col]  = le.transform(test_df[col])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P_emaildomain\n",
            "R_emaildomain\n",
            "id_30\n",
            "id_31\n",
            "DeviceType\n",
            "DeviceInfo\n",
            "DeviceInfo_device\n",
            "DeviceInfo_version\n",
            "id_30_device\n",
            "id_30_version\n",
            "id_31_device\n",
            "groups\n",
            "UserID_proxy\n",
            "uid_DT\n",
            "uid5\n",
            "product_type\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHXoG9y6ZSUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n",
        "                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n",
        "                   'C14', 'D1','D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n",
        "                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V_1_11_0', 'V_1_11_1', 'V_1_11_2', 'V_1_11_3', 'V_1_11_4', 'V_1_11_5', \n",
        "                   'V_1_11_6', 'V_12_34_0', 'V_12_34_1', 'V_12_34_2', 'V_12_34_3', 'V_12_34_4', 'V_12_34_5', 'V_12_34_6', \n",
        "                   'V_12_34_7', 'V_12_34_8', 'V_12_34_9', 'V_12_34_10', 'V_12_34_11', 'V_35_52_0', 'V_35_52_1', 'V_35_52_2', \n",
        "                   'V_35_52_3', 'V_35_52_4', 'V_35_52_5', 'V_35_52_6', 'V_35_52_7', 'V_35_52_8', 'V_53_74_0', 'V_53_74_1', \n",
        "                   'V_53_74_2', 'V_53_74_3', 'V_53_74_4', 'V_53_74_5', 'V_53_74_6', 'V_53_74_7', 'V_53_74_8', 'V_53_74_9', \n",
        "                   'V_53_74_10', 'V_53_74_11', 'V_75_94_0', 'V_75_94_1', 'V_75_94_2', 'V_75_94_3', 'V_75_94_4', 'V_75_94_5', \n",
        "                   'V_75_94_6', 'V_75_94_7', 'V_75_94_8', 'V_75_94_9', 'V_75_94_10', 'V_95_137_0', 'V_95_137_1', 'V_95_137_2', \n",
        "                   'V_138_166_0', 'V_167_216_0', 'V_167_216_1', 'V_167_216_2', 'V_217_278_0', 'V_217_278_1', 'V_217_278_2', \n",
        "                    'V_322_339_0', 'V_322_339_1', 'V_322_339_2', 'V_279_321_0', 'V_279_321_1', 'V_279_321_2', 'V_279_321_3',\n",
        "                   'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n",
        "                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n",
        "                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n",
        "#\n",
        "\n",
        "# useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n",
        "#                    'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n",
        "#                    'C14', 'D1','D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'M2', 'M3',\n",
        "#                    'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V_1_11_0', 'V_1_11_1', 'V_1_11_2', 'V_1_11_3', 'V_1_11_4', 'V_1_11_5', \n",
        "#                    'V_1_11_6', 'V_12_34_0', 'V_12_34_1', 'V_12_34_2', 'V_12_34_3', 'V_12_34_4', 'V_12_34_5', 'V_12_34_6', \n",
        "#                    'V_12_34_7', 'V_12_34_8', 'V_12_34_9', 'V_12_34_10', 'V_12_34_11', 'V_35_52_0', 'V_35_52_1', 'V_35_52_2', \n",
        "#                    'V_35_52_3', 'V_35_52_4', 'V_35_52_5', 'V_35_52_6', 'V_35_52_7', 'V_35_52_8', 'V_53_74_0', 'V_53_74_1', \n",
        "#                    'V_53_74_2', 'V_53_74_3', 'V_53_74_4', 'V_53_74_5', 'V_53_74_6', 'V_53_74_7', 'V_53_74_8', 'V_53_74_9', \n",
        "#                    'V_53_74_10', 'V_53_74_11', 'V_75_94_0', 'V_75_94_1', 'V_75_94_2', 'V_75_94_3', 'V_75_94_4', 'V_75_94_5', \n",
        "#                    'V_75_94_6', 'V_75_94_7', 'V_75_94_8', 'V_75_94_9', 'V_75_94_10', 'V_95_137_0', 'V_95_137_1', 'V_95_137_2', \n",
        "#                    'V_138_166_0', 'V_167_216_0', 'V_167_216_1', 'V_167_216_2', 'V_217_278_0', 'V_217_278_1', 'V_217_278_2', \n",
        "#                    'V_322_339_0', 'V_322_339_1', 'V_322_339_2' \n",
        "#                    'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n",
        "#                    'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_32', 'id_33',\n",
        "#                    'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOcWifAd1YHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features += ['ProductCD'] + \\\n",
        "           [\"card\"+f\"{i+1}\" for i in range(6)] + \\\n",
        "           [\"addr\"+f\"{i+1}\" for i in range(2)] + \\\n",
        "           [\"P_emaildomain\", \"R_emaildomain\"] + \\\n",
        "           [\"DeviceType\", \"DeviceInfo\"] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjtBzHfN0KE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K26kEMPB0J7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myRim6cGZSUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_columns = [col for col in list(train_df) if col not in remove_features]\n",
        "bad_features = [col for col in base_columns if col not in useful_features]\n",
        "bad_features = list(set(bad_features) - set(set(bad_features) - set(train_df.columns)))\n",
        "features_columns = list(set(features_columns) - set(bad_features))\n",
        "\n",
        "\n",
        "categorical_features = [col for col in categorical_features if col in features_columns]\n",
        "categorical_features = list(set(categorical_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQVwkLq5ZSUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train_df.sort_values('TransactionDT')[features_columns].fillna(-10000)\n",
        "y = train_df.sort_values('TransactionDT')['isFraud']\n",
        "dt_m = train_df.sort_values('TransactionDT')['DT_M']\n",
        "\n",
        "X_test = test_df[features_columns].fillna(-10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aizIygF6Fa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(categorical_features)\n",
        "\n",
        "#categorical_features = ['id_02__id_20', 'card2', 'C13', 'id_30_device', 'C8', 'card3', 'id_30_version', 'C2', 'card5__P_emaildomain', 'card6', 'DeviceInfo', 'id_31', 'id_31_device', 'uid_DT', 'C6', 'C1', 'addr2', 'ProductCD', 'card4', 'D8__D9', 'P_emaildomain', 'R_emaildomain', 'C10', 'product_type', 'addr1__card1', 'card5', 'M2__M3', 'id_02__D8', 'id_30', 'addr1', 'uid5', 'C9', 'C12', 'C7', 'C5', 'DeviceInfo__P_emaildomain', 'groups', 'card1', 'card1__card5', 'C11', 'DeviceInfo_version', 'DeviceType', 'id_33', 'D11__DeviceInfo', 'card2__dist1', 'C14', 'card2__id_20', 'C4', 'P_emaildomain__C2', 'DeviceInfo_device']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQEhzkCCupfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#categorical_features = [feat for feat in categorical_features if feat not in ['id_31_device', 'addr1__card1', 'id_02__D8']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL-v2uN0ZSUR",
        "colab_type": "code",
        "outputId": "8fdbc336-8085-4e70-8d8a-674525c4ff6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((590540, 464), (506691, 464))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EipDXPucZSUY",
        "colab_type": "code",
        "outputId": "d66784ee-7449-4499-c3f4-d75301c9587a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = reduce_mem_usage(X)\n",
        "X_test = reduce_mem_usage(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 799.16 Mb (57.3% reduction)\n",
            "Mem. usage decreased to 686.65 Mb (57.2% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXfl30DLZSUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.to_pickle('X.pkl')\n",
        "y.to_pickle('y.pkl')\n",
        "X_test.to_pickle('X_test.pkl')\n",
        "dt_m.to_pickle('dt_m.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU1LqSFsgdV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = train_df.sort_values('TransactionDT')[['TransactionID', 'isFraud', 'DT_M'] + features_columns].fillna(-10000)\n",
        "# test = test_df.sort_values('TransactionDT')[['TransactionID'] + features_columns].fillna(-10000)\n",
        "\n",
        "# train = reduce_mem_usage(train)\n",
        "# test = reduce_mem_usage(test)\n",
        "\n",
        "# train.to_csv('train.csv', index=False)\n",
        "# test.to_csv('test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osDdv4hpZSUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#del train_df, test_df, X_test, train_identity, test_identity, X, y\n",
        "#gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hruY1ZHHo66Y",
        "colab_type": "code",
        "outputId": "81782044-758a-43ba-ab59-c0c87ef06a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.1.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GRnDr1wZSVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "X = pd.read_pickle('X.pkl')\n",
        "y = pd.read_pickle('y.pkl')\n",
        "X_test = pd.read_pickle('X_test.pkl')\n",
        "dt_m = pd.read_pickle('dt_m.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHFxlNbzZSVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_params = {\n",
        "              'n_estimators':2500,\n",
        "              'learning_rate': 0.07,\n",
        "              'eval_metric':'AUC',\n",
        "              'loss_function':'Logloss',\n",
        "              'random_seed': 42,\n",
        "              'metric_period':500,\n",
        "              'od_wait':500,\n",
        "              'task_type':'GPU',\n",
        "              'depth': 8,\n",
        "              } "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "926QZnXRZSVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "NFOLDS = 5\n",
        "folds = GroupKFold(n_splits=NFOLDS)\n",
        "\n",
        "#neptune.create_experiment(params=params)\n",
        "\n",
        "columns = X.columns\n",
        "splits = folds.split(X, y, groups=dt_m)\n",
        "y_preds = np.zeros(X_test.shape[0])\n",
        "y_oof = np.zeros(X.shape[0])\n",
        "score = 0\n",
        "  \n",
        "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
        "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
        "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "    \n",
        "    # Run model for this fold\n",
        "    \n",
        "    estimator = CatBoostClassifier(**cat_params)        \n",
        "    estimator.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_valid, y_valid),\n",
        "        cat_features=categorical_features,\n",
        "        use_best_model=False,\n",
        "        early_stopping_rounds = 5000,\n",
        "        verbose=True)\n",
        "    \n",
        "    y_pred_valid = estimator.predict_proba(X_valid)[:,1]\n",
        "    y_oof[valid_index] = y_pred_valid\n",
        "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
        "    \n",
        "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
        "    y_preds += estimator.predict_proba(X_test)[:,1] / NFOLDS\n",
        "    \n",
        "    del X_train, X_valid, y_train, y_valid\n",
        "    gc.collect()\n",
        "    \n",
        "print(f\"\\nMean AUC = {score}\")\n",
        "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")\n",
        "#neptune.send_metric('local cv', score)\n",
        "#neptune.send_metric('oof score', roc_auc_score(y, y_oof))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4wA-SSGBxun",
        "colab_type": "text"
      },
      "source": [
        "* 9458"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7EiE5LIZSVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#neptune.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnhnt9bGseK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "name = 'g_catboost' + '_' + str(int(score * 10000))\n",
        "path = name\n",
        "\n",
        "os.mkdir(path)\n",
        "\n",
        "sample_submission = pd.read_csv('sample_submission.csv', index_col='TransactionID').reset_index()\n",
        "sample_submission[\"isFraud\"] = y_preds\n",
        "sample_submission.to_csv(name + '/test_' + name +'.csv', index=False)\n",
        "\n",
        "train_oof = pd.DataFrame()\n",
        "train_oof['TransactionID'] = train_df.sort_values('TransactionDT')['TransactionID']\n",
        "train_oof['isFraud'] = y_oof\n",
        "train_oof.to_csv(name + '/train_' + name + '.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrAkCrOYuzo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_oof.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvQhGxGw2B9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}